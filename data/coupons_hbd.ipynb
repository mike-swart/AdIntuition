{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import urllib\n",
    "from scipy.stats import mannwhitneyu\n",
    "import re\n",
    "import os.path\n",
    "from langdetect import detect\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import fastcluster\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "import sys\n",
    "from scipy.cluster.hierarchy import fcluster, dendrogram\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import gensim \n",
    "import hdbscan\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.setrecursionlimit(100000)\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube - Exploratory Analyses\n",
    "### Loading the dataset\n",
    "First, let us load the dataset from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('youtube.db')\n",
    "video = pd.read_sql_query('''SELECT v.autoId as autoId, \n",
    "                                    v.id as id,\n",
    "                                    v.categoryId as categoryId,\n",
    "                                    v.channelId as channelId,\n",
    "                                    v.publishedAt as publishedAt,\n",
    "                                    v.title as title,\n",
    "                                    v.description as description,\n",
    "                                    v.viewCount as viewCount,\n",
    "                                    v.likeCount as likeCount,\n",
    "                                    v.dislikeCount as dislikeCount,\n",
    "                                    v.favoriteCount as favoriteCount,\n",
    "                                    v.commentCount as commentCount,\n",
    "                                    v.duration as duration,\n",
    "                                    v.defaultLanguage as defaultLanguage,\n",
    "                                    c.title as channelTitle,\n",
    "                                    c.description as channelDescription,\n",
    "                                    c.publishedAt as channelPublishedAt,\n",
    "                                    c.viewCount as channelViewCount,\n",
    "                                    c.commentCount as channelCommentCount,\n",
    "                                    c.subscriberCount as channelSubscriberCount,\n",
    "                                    c.videoCount as channelVideoCount,\n",
    "                                    c.country as channelCountry\n",
    "                                    from video v left join channel c on v.channelId = c.id limit 130000''', con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's examine the count of videos we collected. The tables are ``video``, ``url``, ``urlResolve``, and ``category``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130000, 22)\n"
     ]
    }
   ],
   "source": [
    "print video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autoId',\n",
       " 'id',\n",
       " 'categoryId',\n",
       " 'channelId',\n",
       " 'publishedAt',\n",
       " 'title',\n",
       " 'description',\n",
       " 'viewCount',\n",
       " 'likeCount',\n",
       " 'dislikeCount',\n",
       " 'favoriteCount',\n",
       " 'commentCount',\n",
       " 'duration',\n",
       " 'defaultLanguage',\n",
       " 'channelTitle',\n",
       " 'channelDescription',\n",
       " 'channelPublishedAt',\n",
       " 'channelViewCount',\n",
       " 'channelCommentCount',\n",
       " 'channelSubscriberCount',\n",
       " 'channelVideoCount',\n",
       " 'channelCountry']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(video.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130000, 22)\n"
     ]
    }
   ],
   "source": [
    "print video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the affiliate video descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What languages are these descriptions in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en         44354\n",
      "Unknown    38227\n",
      "es          6506\n",
      "pt          5044\n",
      "ru          4035\n",
      "de          3323\n",
      "ja          2737\n",
      "fr          2730\n",
      "ko          2194\n",
      "ar          1936\n",
      "it          1924\n",
      "id          1566\n",
      "tr          1155\n",
      "ca          1067\n",
      "ro          1033\n",
      "vi          1027\n",
      "nl           936\n",
      "th           876\n",
      "pl           874\n",
      "tl           706\n",
      "so           641\n",
      "sv           549\n",
      "et           500\n",
      "af           479\n",
      "no           478\n",
      "da           421\n",
      "hr           401\n",
      "bg           394\n",
      "cy           354\n",
      "hu           343\n",
      "fi           332\n",
      "sw           330\n",
      "sl           271\n",
      "cs           244\n",
      "el           224\n",
      "he           214\n",
      "zh-cn        188\n",
      "uk           187\n",
      "lt           184\n",
      "sk           164\n",
      "sq           158\n",
      "zh-tw        143\n",
      "mk           112\n",
      "fa           107\n",
      "bn            79\n",
      "lv            78\n",
      "hi            63\n",
      "ur            30\n",
      "ta            24\n",
      "ne            15\n",
      "ml            14\n",
      "te            13\n",
      "mr            11\n",
      "pa             2\n",
      "gu             2\n",
      "kn             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_language(x):\n",
    "    language = 'Unknown'\n",
    "    try:\n",
    "        language = detect(x.description.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return language\n",
    "\n",
    "vids = video.apply(get_language, axis=1)\n",
    "print vids.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_en = video[vids == 'en'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44354, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_en.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43653, 3)\n"
     ]
    }
   ],
   "source": [
    "#could write to csv\n",
    "# with open('videos_en.tsv', 'wb') as csvfile:\n",
    "#     writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "#     count = 0\n",
    "#     for index, row in videos_en.iterrows():\n",
    "#         try:\n",
    "#             writer.writerow([row[\"id\"].encode('utf-8'), row[\"description\"].encode('utf-8'), row[\"channelTitle\"].encode('utf-8')])\n",
    "#         except:\n",
    "#             count += 1\n",
    "#     print count\n",
    "\n",
    "videos = []\n",
    "with open(\"videos_en.tsv\", 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        videos.append([row[0], row[1], row[2]])\n",
    "        \n",
    "videos_en = pd.DataFrame(videos, columns=['id', 'description', 'channelTitle'])\n",
    "print videos_en.shape\n",
    "videos = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize(line):\n",
    "    if (line is None):\n",
    "        line = ''\n",
    "    printable = set(string.printable)\n",
    "    line = ''.join(filter(lambda x: x in printable, line)) \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]*\\'[a-zA-Z]*|\\w+')\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    line = re.sub(r'(http[s]?://|www.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]*|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))*', '', line).lower()\n",
    "    tokens.extend(tokenizer.tokenize(line))\n",
    "    \n",
    "    tokens_ = [f.strip(string.punctuation) for f in tokens]\n",
    "    tokens_ = [f for f in tokens_ if f != '' and f not in stopwords and len(f) != 1]\n",
    "    tokens_ = [f for f in tokens_ if not (f.isdigit() or f[0] == '-' and f[1:].isdigit())]\n",
    "    tokens_ = [stemmer.stem(f) for f in tokens_]\n",
    "\n",
    "    return tokens_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230749, 4)\n",
      "871\n"
     ]
    }
   ],
   "source": [
    "# Add a sentence number value here\n",
    "rows = []\n",
    "\n",
    "def description_parse(desc):\n",
    "    sentences = []\n",
    "    for line in desc.splitlines():\n",
    "        for sent in sent_tokenize(line):\n",
    "            sentences.append(sent)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "count = 0\n",
    "for index, row in videos_en.iterrows():\n",
    "    try:\n",
    "        sentences = description_parse(row.description)\n",
    "        for sent in sentences:\n",
    "            if len(tokenize(sent)) != 0:\n",
    "                rows.append([row['id'], row['channelTitle'], row['description'], sent])\n",
    "    except:\n",
    "        count+=1\n",
    "\n",
    "videos_en_new = pd.DataFrame(rows, columns=['id', 'channelTitle', 'description', 'sentence'])\n",
    "videos_en = [] #clear this from memory\n",
    "print videos_en_new.shape\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100001, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_en_new1 = videos_en_new.copy().loc[:50000]\n",
    "videos_en_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec = CountVectorizer(tokenizer=tokenize, binary=True).fit(videos_en_new1['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineVec = countVec.transform(videos_en_new['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True).fit(lineVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True, \n",
    "        gen_min_span_tree=True, leaf_size=40, memory=Memory(cachedir=None),\n",
    "        metric='cosine', min_cluster_size=5, min_samples=None, p=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfIdfMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = cosine_distances(lineVec)\n",
    "#pass tfIdfMatrix instead of lineVec\n",
    "#look on sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distVec = squareform(dist, checks = False)\n",
    "dist = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fastcluster.linkage(distVec, method = 'ward', preserve_input = False)\n",
    "distVec = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35, 10))\n",
    "plt.xlabel('Line Index')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    res,\n",
    "    leaf_rotation=90.,  \n",
    "    leaf_font_size=8.,  \n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_en_new['ward_cosine_cluster'] = fcluster(res, 5, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('videos_en_new.tsv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "    count = 0\n",
    "    for index, row in videos_en.iterrows():\n",
    "        try:\n",
    "            writer.writerow([row[\"id\"].encode('utf-8'), row[\"sentence\"].encode('utf-8'), row[\"channelTitle\"].encode('utf-8'), row.ward_cosine_cluster])\n",
    "        except:\n",
    "            count += 1\n",
    "    print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for index, row in videos_en_new.iterrows():\n",
    "    try:\n",
    "        str(row.sentence).index(\"use code\")\n",
    "        print row.sentence\n",
    "        print row.ward_cosine_cluster \n",
    "        print\n",
    "        count += 1\n",
    "    except:\n",
    "        x = re.findall(\"[0-9]?[0-9]% off\", row.sentence)\n",
    "        if x:\n",
    "            print row.sentence\n",
    "            print row.ward_cosine_cluster\n",
    "        x = re.findall(\"\\$[0-9]?[0-9]\", row.sentence)\n",
    "        if x:\n",
    "            print row.sentence\n",
    "            print row.ward_cosine_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to be changed on a re-run\n",
    "cluster_numbers = [17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in videos_en_new.iterrows():\n",
    "#     if row.ward_cosine_cluster in [677]:\n",
    "#         print row.sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "\n",
    "for index, row in videos_en_new.iterrows():\n",
    "    if row.ward_cosine_cluster in cluster_numbers:\n",
    "        if row.id not in info:\n",
    "             info[row.id] = [row.channelTitle, []]\n",
    "        info[row.id][1].append(row.sentence)\n",
    "\n",
    "for vidId in info.keys():\n",
    "    try:\n",
    "        print info[vidId][0]\n",
    "        for item in info[vidId][1]:\n",
    "            print \"\\t\" + item\n",
    "        print\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVec1 = CountVectorizer(tokenizer=tokenize, binary=True).fit_transform(videos_en_new['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = countVec.get_feature_names()\n",
    "for i in range(len(arr)):\n",
    "    if arr[i] == \"code\":\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = countVec1.toarray()\n",
    "indices = []\n",
    "for i in range(len(arr1)):\n",
    "    if arr1[i][3270] == 1:\n",
    "        print i\n",
    "        indices.append(i)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_distances = []\n",
    "distances_indices = []\n",
    "for index, row in videos_en_new.iterrows():\n",
    "    if row.ward_cosine_cluster in cluster_numbers and len(row.sentence) > 20:\n",
    "        index_distances.append(arr1[index])\n",
    "        distances_indices.append(index)\n",
    "\n",
    "average = []\n",
    "for i in range(len(index_distances[0])):\n",
    "    average.append(0)\n",
    "    for array in index_distances:\n",
    "        average[i] += array[i]\n",
    "    average[i] /= (len(index_distances) *1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_from_sentence(sentence):\n",
    "    ret_val = [0] * len(arr1[0])\n",
    "    \n",
    "    split = re.split('\\W+', sentence)\n",
    "    for word in split:\n",
    "        if word in arr:\n",
    "            ret_val[arr.index(word)] = 1\n",
    "    return ret_val  \n",
    "\n",
    "def get_total_distance(sentence):\n",
    "    array = list(index_distances)\n",
    "    array.append(get_array_from_sentence(sentence))\n",
    "    return cosine_distances(array)\n",
    "\n",
    "def get_closest(sentence):\n",
    "    twod = get_total_distance(sentence)\n",
    "    vals = twod[len(twod)-1]\n",
    "    least = 100\n",
    "    least_index = -1\n",
    "    for i in range(len(vals)-1):\n",
    "        if least >= vals[i]:\n",
    "            least = vals[i]\n",
    "            least_index = i\n",
    "    print least, \n",
    "    print \"\\t\",\n",
    "    loc = distances_indices[least_index]\n",
    "    print videos_en_new.loc[loc].sentence\n",
    "\n",
    "get_closest(\"use my code in the chat please\")\n",
    "get_closest(\"use code \\\"turtle\\\" for 25% off at\")\n",
    "get_closest(\"get 20% using code \\\"turle\\\"\")\n",
    "get_closest(\"save $20 using code \\\"turle\\\"\")\n",
    "get_closest(\"I like turtles and potatoes\")\n",
    "get_closest(\"I like to code during my free time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for index, row in videos_en_new.iterrows():\n",
    "    if row.ward_cosine_cluster in cluster_numbers:\n",
    "        sentence_array = []\n",
    "        for word in word_tokenize(row.sentence):\n",
    "            x = re.findall(\"\\$?[0-9]?[0-9]%?\", row.sentence)\n",
    "            if x:\n",
    "                sentence_array.append(\"15\")\n",
    "            else:\n",
    "                sentence_array.append(word.lower())\n",
    "        data.append(sentence_array)\n",
    "\n",
    "# data.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [\"use my code in the chat please\", \"use code \\\"turtle\\\" for 25% off at\", \"get 20% off using code \\\"turle\\\"\",\n",
    " \"save $20 using code \\\"turle\\\"\", \"I like turtles and potatoes\", \"I like to code during my free time\"]\n",
    "\n",
    "for sentence in temp:\n",
    "    avg = 0\n",
    "    count = 0\n",
    "    words = word_tokenize(sentence)\n",
    "    for i in range(len(words)-1):\n",
    "        try:\n",
    "            a = words[i]\n",
    "            b = words[i+1]\n",
    "            if re.findall(\"\\$?[0-9]?[0-9]%?\", a):\n",
    "                a = \"15\"\n",
    "            if re.findall(\"\\$?[0-9]?[0-9]%?\", b):\n",
    "                b = \"15\"\n",
    "            val = math.fabs(model.similarity(a, b))\n",
    "            #print val\n",
    "            avg += val\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if count > 0:\n",
    "        print avg/count,\n",
    "    else:\n",
    "        print \"\\t\",\n",
    "    print \"\\t\",\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_doc = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent(sent):\n",
    "    new_sentence = sent.split(\" \")  \n",
    "    c = model_doc.infer_vector(new_sentence)\n",
    "    print model_doc.docvecs.most_similar(positive=[c],topn=1)\n",
    "    \n",
    "for sent in temp:\n",
    "    check_sent(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
